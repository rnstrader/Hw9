[
  {
    "objectID": "Hw9.html",
    "href": "Hw9.html",
    "title": "Hw8",
    "section": "",
    "text": "Reading in the Seoul bike sharing data set, using locale argument to fix error with element 1.\n\nlibrary(tidyverse)\n\nWarning: package 'stringr' was built under R version 4.5.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nbike &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\", locale=locale(encoding=\"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hw9.html#reading-data",
    "href": "Hw9.html#reading-data",
    "title": "Hw8",
    "section": "",
    "text": "Reading in the Seoul bike sharing data set, using locale argument to fix error with element 1.\n\nlibrary(tidyverse)\n\nWarning: package 'stringr' was built under R version 4.5.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nbike &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\", locale=locale(encoding=\"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hw9.html#eda",
    "href": "Hw9.html#eda",
    "title": "Hw8",
    "section": "EDA",
    "text": "EDA\n\nChecking the Data\nTo start, we need to check our data set and prepare it for use in generating models. First we check for any missing values.\n\ncolSums(is.na(bike))\n\n                     Date         Rented Bike Count                      Hour \n                        0                         0                         0 \n          Temperature(°C)               Humidity(%)          Wind speed (m/s) \n                        0                         0                         0 \n         Visibility (10m) Dew point temperature(°C)   Solar Radiation (MJ/m2) \n                        0                         0                         0 \n             Rainfall(mm)             Snowfall (cm)                   Seasons \n                        0                         0                         0 \n                  Holiday           Functioning Day \n                        0                         0 \n\n\nIt appears there are no missing values in the dataset. Next we will check the types of data in each column, as well as show summary stats for each numeric column and levels for categorical columns.\n\nstr(bike)\n\nspc_tbl_ [8,760 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Date                     : chr [1:8760] \"01/12/2017\" \"01/12/2017\" \"01/12/2017\" \"01/12/2017\" ...\n $ Rented Bike Count        : num [1:8760] 254 204 173 107 78 100 181 460 930 490 ...\n $ Hour                     : num [1:8760] 0 1 2 3 4 5 6 7 8 9 ...\n $ Temperature(°C)          : num [1:8760] -5.2 -5.5 -6 -6.2 -6 -6.4 -6.6 -7.4 -7.6 -6.5 ...\n $ Humidity(%)              : num [1:8760] 37 38 39 40 36 37 35 38 37 27 ...\n $ Wind speed (m/s)         : num [1:8760] 2.2 0.8 1 0.9 2.3 1.5 1.3 0.9 1.1 0.5 ...\n $ Visibility (10m)         : num [1:8760] 2000 2000 2000 2000 2000 ...\n $ Dew point temperature(°C): num [1:8760] -17.6 -17.6 -17.7 -17.6 -18.6 -18.7 -19.5 -19.3 -19.8 -22.4 ...\n $ Solar Radiation (MJ/m2)  : num [1:8760] 0 0 0 0 0 0 0 0 0.01 0.23 ...\n $ Rainfall(mm)             : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ Snowfall (cm)            : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ Seasons                  : chr [1:8760] \"Winter\" \"Winter\" \"Winter\" \"Winter\" ...\n $ Holiday                  : chr [1:8760] \"No Holiday\" \"No Holiday\" \"No Holiday\" \"No Holiday\" ...\n $ Functioning Day          : chr [1:8760] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Date = col_character(),\n  ..   `Rented Bike Count` = col_double(),\n  ..   Hour = col_double(),\n  ..   `Temperature(°C)` = col_double(),\n  ..   `Humidity(%)` = col_double(),\n  ..   `Wind speed (m/s)` = col_double(),\n  ..   `Visibility (10m)` = col_double(),\n  ..   `Dew point temperature(°C)` = col_double(),\n  ..   `Solar Radiation (MJ/m2)` = col_double(),\n  ..   `Rainfall(mm)` = col_double(),\n  ..   `Snowfall (cm)` = col_double(),\n  ..   Seasons = col_character(),\n  ..   Holiday = col_character(),\n  ..   `Functioning Day` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(bike)\n\n     Date           Rented Bike Count      Hour       Temperature(°C) \n Length:8760        Min.   :   0.0    Min.   : 0.00   Min.   :-17.80  \n Class :character   1st Qu.: 191.0    1st Qu.: 5.75   1st Qu.:  3.50  \n Mode  :character   Median : 504.5    Median :11.50   Median : 13.70  \n                    Mean   : 704.6    Mean   :11.50   Mean   : 12.88  \n                    3rd Qu.:1065.2    3rd Qu.:17.25   3rd Qu.: 22.50  \n                    Max.   :3556.0    Max.   :23.00   Max.   : 39.40  \n  Humidity(%)    Wind speed (m/s) Visibility (10m) Dew point temperature(°C)\n Min.   : 0.00   Min.   :0.000    Min.   :  27     Min.   :-30.600          \n 1st Qu.:42.00   1st Qu.:0.900    1st Qu.: 940     1st Qu.: -4.700          \n Median :57.00   Median :1.500    Median :1698     Median :  5.100          \n Mean   :58.23   Mean   :1.725    Mean   :1437     Mean   :  4.074          \n 3rd Qu.:74.00   3rd Qu.:2.300    3rd Qu.:2000     3rd Qu.: 14.800          \n Max.   :98.00   Max.   :7.400    Max.   :2000     Max.   : 27.200          \n Solar Radiation (MJ/m2)  Rainfall(mm)     Snowfall (cm)       Seasons         \n Min.   :0.0000          Min.   : 0.0000   Min.   :0.00000   Length:8760       \n 1st Qu.:0.0000          1st Qu.: 0.0000   1st Qu.:0.00000   Class :character  \n Median :0.0100          Median : 0.0000   Median :0.00000   Mode  :character  \n Mean   :0.5691          Mean   : 0.1487   Mean   :0.07507                     \n 3rd Qu.:0.9300          3rd Qu.: 0.0000   3rd Qu.:0.00000                     \n Max.   :3.5200          Max.   :35.0000   Max.   :8.80000                     \n   Holiday          Functioning Day   \n Length:8760        Length:8760       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\nlapply(bike, function(x) if(is.character(x) || is.factor(x)) unique(x))\n\n$Date\n  [1] \"01/12/2017\" \"02/12/2017\" \"03/12/2017\" \"04/12/2017\" \"05/12/2017\"\n  [6] \"06/12/2017\" \"07/12/2017\" \"08/12/2017\" \"09/12/2017\" \"10/12/2017\"\n [11] \"11/12/2017\" \"12/12/2017\" \"13/12/2017\" \"14/12/2017\" \"15/12/2017\"\n [16] \"16/12/2017\" \"17/12/2017\" \"18/12/2017\" \"19/12/2017\" \"20/12/2017\"\n [21] \"21/12/2017\" \"22/12/2017\" \"23/12/2017\" \"24/12/2017\" \"25/12/2017\"\n [26] \"26/12/2017\" \"27/12/2017\" \"28/12/2017\" \"29/12/2017\" \"30/12/2017\"\n [31] \"31/12/2017\" \"01/01/2018\" \"02/01/2018\" \"03/01/2018\" \"04/01/2018\"\n [36] \"05/01/2018\" \"06/01/2018\" \"07/01/2018\" \"08/01/2018\" \"09/01/2018\"\n [41] \"10/01/2018\" \"11/01/2018\" \"12/01/2018\" \"13/01/2018\" \"14/01/2018\"\n [46] \"15/01/2018\" \"16/01/2018\" \"17/01/2018\" \"18/01/2018\" \"19/01/2018\"\n [51] \"20/01/2018\" \"21/01/2018\" \"22/01/2018\" \"23/01/2018\" \"24/01/2018\"\n [56] \"25/01/2018\" \"26/01/2018\" \"27/01/2018\" \"28/01/2018\" \"29/01/2018\"\n [61] \"30/01/2018\" \"31/01/2018\" \"01/02/2018\" \"02/02/2018\" \"03/02/2018\"\n [66] \"04/02/2018\" \"05/02/2018\" \"06/02/2018\" \"07/02/2018\" \"08/02/2018\"\n [71] \"09/02/2018\" \"10/02/2018\" \"11/02/2018\" \"12/02/2018\" \"13/02/2018\"\n [76] \"14/02/2018\" \"15/02/2018\" \"16/02/2018\" \"17/02/2018\" \"18/02/2018\"\n [81] \"19/02/2018\" \"20/02/2018\" \"21/02/2018\" \"22/02/2018\" \"23/02/2018\"\n [86] \"24/02/2018\" \"25/02/2018\" \"26/02/2018\" \"27/02/2018\" \"28/02/2018\"\n [91] \"01/03/2018\" \"02/03/2018\" \"03/03/2018\" \"04/03/2018\" \"05/03/2018\"\n [96] \"06/03/2018\" \"07/03/2018\" \"08/03/2018\" \"09/03/2018\" \"10/03/2018\"\n[101] \"11/03/2018\" \"12/03/2018\" \"13/03/2018\" \"14/03/2018\" \"15/03/2018\"\n[106] \"16/03/2018\" \"17/03/2018\" \"18/03/2018\" \"19/03/2018\" \"20/03/2018\"\n[111] \"21/03/2018\" \"22/03/2018\" \"23/03/2018\" \"24/03/2018\" \"25/03/2018\"\n[116] \"26/03/2018\" \"27/03/2018\" \"28/03/2018\" \"29/03/2018\" \"30/03/2018\"\n[121] \"31/03/2018\" \"01/04/2018\" \"02/04/2018\" \"03/04/2018\" \"04/04/2018\"\n[126] \"05/04/2018\" \"06/04/2018\" \"07/04/2018\" \"08/04/2018\" \"09/04/2018\"\n[131] \"10/04/2018\" \"11/04/2018\" \"12/04/2018\" \"13/04/2018\" \"14/04/2018\"\n[136] \"15/04/2018\" \"16/04/2018\" \"17/04/2018\" \"18/04/2018\" \"19/04/2018\"\n[141] \"20/04/2018\" \"21/04/2018\" \"22/04/2018\" \"23/04/2018\" \"24/04/2018\"\n[146] \"25/04/2018\" \"26/04/2018\" \"27/04/2018\" \"28/04/2018\" \"29/04/2018\"\n[151] \"30/04/2018\" \"01/05/2018\" \"02/05/2018\" \"03/05/2018\" \"04/05/2018\"\n[156] \"05/05/2018\" \"06/05/2018\" \"07/05/2018\" \"08/05/2018\" \"09/05/2018\"\n[161] \"10/05/2018\" \"11/05/2018\" \"12/05/2018\" \"13/05/2018\" \"14/05/2018\"\n[166] \"15/05/2018\" \"16/05/2018\" \"17/05/2018\" \"18/05/2018\" \"19/05/2018\"\n[171] \"20/05/2018\" \"21/05/2018\" \"22/05/2018\" \"23/05/2018\" \"24/05/2018\"\n[176] \"25/05/2018\" \"26/05/2018\" \"27/05/2018\" \"28/05/2018\" \"29/05/2018\"\n[181] \"30/05/2018\" \"31/05/2018\" \"01/06/2018\" \"02/06/2018\" \"03/06/2018\"\n[186] \"04/06/2018\" \"05/06/2018\" \"06/06/2018\" \"07/06/2018\" \"08/06/2018\"\n[191] \"09/06/2018\" \"10/06/2018\" \"11/06/2018\" \"12/06/2018\" \"13/06/2018\"\n[196] \"14/06/2018\" \"15/06/2018\" \"16/06/2018\" \"17/06/2018\" \"18/06/2018\"\n[201] \"19/06/2018\" \"20/06/2018\" \"21/06/2018\" \"22/06/2018\" \"23/06/2018\"\n[206] \"24/06/2018\" \"25/06/2018\" \"26/06/2018\" \"27/06/2018\" \"28/06/2018\"\n[211] \"29/06/2018\" \"30/06/2018\" \"01/07/2018\" \"02/07/2018\" \"03/07/2018\"\n[216] \"04/07/2018\" \"05/07/2018\" \"06/07/2018\" \"07/07/2018\" \"08/07/2018\"\n[221] \"09/07/2018\" \"10/07/2018\" \"11/07/2018\" \"12/07/2018\" \"13/07/2018\"\n[226] \"14/07/2018\" \"15/07/2018\" \"16/07/2018\" \"17/07/2018\" \"18/07/2018\"\n[231] \"19/07/2018\" \"20/07/2018\" \"21/07/2018\" \"22/07/2018\" \"23/07/2018\"\n[236] \"24/07/2018\" \"25/07/2018\" \"26/07/2018\" \"27/07/2018\" \"28/07/2018\"\n[241] \"29/07/2018\" \"30/07/2018\" \"31/07/2018\" \"01/08/2018\" \"02/08/2018\"\n[246] \"03/08/2018\" \"04/08/2018\" \"05/08/2018\" \"06/08/2018\" \"07/08/2018\"\n[251] \"08/08/2018\" \"09/08/2018\" \"10/08/2018\" \"11/08/2018\" \"12/08/2018\"\n[256] \"13/08/2018\" \"14/08/2018\" \"15/08/2018\" \"16/08/2018\" \"17/08/2018\"\n[261] \"18/08/2018\" \"19/08/2018\" \"20/08/2018\" \"21/08/2018\" \"22/08/2018\"\n[266] \"23/08/2018\" \"24/08/2018\" \"25/08/2018\" \"26/08/2018\" \"27/08/2018\"\n[271] \"28/08/2018\" \"29/08/2018\" \"30/08/2018\" \"31/08/2018\" \"01/09/2018\"\n[276] \"02/09/2018\" \"03/09/2018\" \"04/09/2018\" \"05/09/2018\" \"06/09/2018\"\n[281] \"07/09/2018\" \"08/09/2018\" \"09/09/2018\" \"10/09/2018\" \"11/09/2018\"\n[286] \"12/09/2018\" \"13/09/2018\" \"14/09/2018\" \"15/09/2018\" \"16/09/2018\"\n[291] \"17/09/2018\" \"18/09/2018\" \"19/09/2018\" \"20/09/2018\" \"21/09/2018\"\n[296] \"22/09/2018\" \"23/09/2018\" \"24/09/2018\" \"25/09/2018\" \"26/09/2018\"\n[301] \"27/09/2018\" \"28/09/2018\" \"29/09/2018\" \"30/09/2018\" \"01/10/2018\"\n[306] \"02/10/2018\" \"03/10/2018\" \"04/10/2018\" \"05/10/2018\" \"06/10/2018\"\n[311] \"07/10/2018\" \"08/10/2018\" \"09/10/2018\" \"10/10/2018\" \"11/10/2018\"\n[316] \"12/10/2018\" \"13/10/2018\" \"14/10/2018\" \"15/10/2018\" \"16/10/2018\"\n[321] \"17/10/2018\" \"18/10/2018\" \"19/10/2018\" \"20/10/2018\" \"21/10/2018\"\n[326] \"22/10/2018\" \"23/10/2018\" \"24/10/2018\" \"25/10/2018\" \"26/10/2018\"\n[331] \"27/10/2018\" \"28/10/2018\" \"29/10/2018\" \"30/10/2018\" \"31/10/2018\"\n[336] \"01/11/2018\" \"02/11/2018\" \"03/11/2018\" \"04/11/2018\" \"05/11/2018\"\n[341] \"06/11/2018\" \"07/11/2018\" \"08/11/2018\" \"09/11/2018\" \"10/11/2018\"\n[346] \"11/11/2018\" \"12/11/2018\" \"13/11/2018\" \"14/11/2018\" \"15/11/2018\"\n[351] \"16/11/2018\" \"17/11/2018\" \"18/11/2018\" \"19/11/2018\" \"20/11/2018\"\n[356] \"21/11/2018\" \"22/11/2018\" \"23/11/2018\" \"24/11/2018\" \"25/11/2018\"\n[361] \"26/11/2018\" \"27/11/2018\" \"28/11/2018\" \"29/11/2018\" \"30/11/2018\"\n\n$`Rented Bike Count`\nNULL\n\n$Hour\nNULL\n\n$`Temperature(°C)`\nNULL\n\n$`Humidity(%)`\nNULL\n\n$`Wind speed (m/s)`\nNULL\n\n$`Visibility (10m)`\nNULL\n\n$`Dew point temperature(°C)`\nNULL\n\n$`Solar Radiation (MJ/m2)`\nNULL\n\n$`Rainfall(mm)`\nNULL\n\n$`Snowfall (cm)`\nNULL\n\n$Seasons\n[1] \"Winter\" \"Spring\" \"Summer\" \"Autumn\"\n\n$Holiday\n[1] \"No Holiday\" \"Holiday\"   \n\n$`Functioning Day`\n[1] \"Yes\" \"No\" \n\n\nNow that we have a good idea of the structure of the data as well as the kind of values we see for both numeric and categorical variables, lets convert the Date column to the Date type of data.\n\nbike &lt;- bike |&gt; mutate(Date = dmy(Date))\n\nThe Date column is now a Date type of data. Now we should convert the other character data columns into factor data.\n\nbike &lt;- bike |&gt; mutate(across(where(is.character), as.factor))\n\nNow that we have categorical variables as factors, our last step to tidy the dataset will be to rename the columns into lower snake case for ease of use, the clean_names function from the janitor package does this automatically.\n\nlibrary(janitor)\n\nWarning: package 'janitor' was built under R version 4.5.2\n\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nbike &lt;- bike |&gt; clean_names()\nnames(bike)\n\n [1] \"date\"                    \"rented_bike_count\"      \n [3] \"hour\"                    \"temperature_c\"          \n [5] \"humidity_percent\"        \"wind_speed_m_s\"         \n [7] \"visibility_10m\"          \"dew_point_temperature_c\"\n [9] \"solar_radiation_mj_m2\"   \"rainfall_mm\"            \n[11] \"snowfall_cm\"             \"seasons\"                \n[13] \"holiday\"                 \"functioning_day\"        \n\n\nThe data is now cleaned nicely, so lets take a look at some summary statistics for bike rental count and bike rental count subsetted by categorical variable levels.\n\nsummary(bike$rented_bike_count)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   191.0   504.5   704.6  1065.2  3556.0 \n\nbike |&gt; group_by(seasons) |&gt;\n  summarize(mean_bike_count = mean(rented_bike_count))\n\n# A tibble: 4 × 2\n  seasons mean_bike_count\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Autumn             820.\n2 Spring             730.\n3 Summer            1034.\n4 Winter             226.\n\nbike |&gt; group_by(holiday) |&gt;\n  summarize(mean_bike_count = mean(rented_bike_count))\n\n# A tibble: 2 × 2\n  holiday    mean_bike_count\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Holiday               500.\n2 No Holiday            715.\n\nbike |&gt; group_by(functioning_day) |&gt;\n  summarize(mean_bike_count = mean(rented_bike_count))\n\n# A tibble: 2 × 2\n  functioning_day mean_bike_count\n  &lt;fct&gt;                     &lt;dbl&gt;\n1 No                           0 \n2 Yes                        729.\n\n\nThis investigation has revealed some interesting temporal patterns in the data, as well as very high extreme values of bike rentals, but most interesting is that no bikes are rented on days that the value of Functioning Day is “No”. This means that we should subset the data to only include days where the bike sharing system is functional.\n\nbike &lt;- bike |&gt; filter(functioning_day == \"Yes\")\n\nNext, in order to simplify future analyses of this data, the rented bike values will be summarized by day to give one value per day, as opposed to one value every hour each day.\n\ndaily_bike &lt;- bike |&gt; group_by(date, seasons, holiday) |&gt; summarize(\n  total_bike_count = sum(rented_bike_count), \n  total_rainfall = sum(rainfall_mm),\n  total_snowfall = sum(snowfall_cm),\n  mean_temperature = mean(temperature_c),\n  mean_humidity = mean(humidity_percent),\n  mean_windspeed = mean(wind_speed_m_s),\n  mean_visibility = mean(visibility_10m),\n  mean_dewpoint = mean(dew_point_temperature_c),\n  mean_solar_radiation = mean(solar_radiation_mj_m2)\n) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\nhead(daily_bike)\n\n# A tibble: 6 × 12\n  date       seasons holiday    total_bike_count total_rainfall total_snowfall\n  &lt;date&gt;     &lt;fct&gt;   &lt;fct&gt;                 &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 2017-12-01 Winter  No Holiday             9539            0              0  \n2 2017-12-02 Winter  No Holiday             8523            0              0  \n3 2017-12-03 Winter  No Holiday             7222            4              0  \n4 2017-12-04 Winter  No Holiday             8729            0.1            0  \n5 2017-12-05 Winter  No Holiday             8307            0              0  \n6 2017-12-06 Winter  No Holiday             6669            1.3            8.6\n# ℹ 6 more variables: mean_temperature &lt;dbl&gt;, mean_humidity &lt;dbl&gt;,\n#   mean_windspeed &lt;dbl&gt;, mean_visibility &lt;dbl&gt;, mean_dewpoint &lt;dbl&gt;,\n#   mean_solar_radiation &lt;dbl&gt;\n\n\nNow that we have a transformed data set that will be easier to analyze, lets recreate the summary stats from earlier for our new dataset. Then lets create some plots to explore if the relationships we might expect from the data are actually visible when plotted.\n\n#Summary Stats for new dataset\nsummary(daily_bike$total_bike_count)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    977    6967   18563   17485   26285   36149 \n\ndaily_bike |&gt; group_by(seasons) |&gt;\n  summarize(mean_bike_count = mean(total_bike_count))\n\n# A tibble: 4 × 2\n  seasons mean_bike_count\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Autumn           22099.\n2 Spring           17910.\n3 Summer           24818.\n4 Winter            5413.\n\ndaily_bike |&gt; group_by(holiday) |&gt; \n  summarize(mean_bike_count = mean(total_bike_count))\n\n# A tibble: 2 × 2\n  holiday    mean_bike_count\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Holiday             12700.\n2 No Holiday          17727.\n\n#Plots to explore possible meteorological and temporal effects on rental bike use\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.5.2\n\ndaily_bike |&gt; ggplot(aes(x = total_rainfall, y = total_bike_count)) + geom_point() + stat_cor(method = \"pearson\", label.x.npc = 0.25) + facet_wrap(~seasons) + theme_minimal() + labs(title = \"Rented Bike Count vs Rainfall\", x = \"Total Rainfall (mm)\", y = \"Rented Bike Count\")\n\n\n\n\n\n\n\ndaily_bike |&gt; ggplot(aes(x = mean_temperature, y = total_bike_count)) + geom_point() + stat_cor(method = \"pearson\") + theme_minimal() + labs(title = \"Rented Bike Count vs Average Temperature\", x = \"Average Temperature (Celsius)\", y = \"Rented Bike Count\")\n\n\n\n\n\n\n\ndaily_bike |&gt; ggplot(aes(x = seasons, y = total_bike_count, fill = holiday)) + geom_col() + theme_minimal() + labs(title = \"Rented Bike Count vs Season By Holiday\", x = \"Season\", y = \"Rented Bike Count\")\n\n\n\n\n\n\n\n\nBased on these explorations, some assumptions made about possible relationships we might see have been confirmed. Regardless of season, bike usage was low during high rainfall events, while its usage during dry periods was more variable. Seasonally, winter sees considerably less use of rental bikes than any other season, and summer has the most use of rental bikes. This is supported by the fact that rental bike use generally appears to increase with warmer temperatures, although we can see a possible indication that very warm weather (&gt; 25 degrees Celsius) might actually see decrease in bike use as temperature increases, which indicates that the ideal temperature of bike use is likely around room temperature, or 22 degrees C. It is important to note with this that the variation of bike usage appears very scattered around this same value, which could be the effect of precipitation or other factors independent of temperature.\n\n\nSplit the Data\nBefore we start modeling, we will need to split the data into training and test set. We will use a 75/25 split of training and test set respectively while stratifying the split by seasons.\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.5.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.2     ✔ tailor       0.1.0\n✔ infer        1.0.9     ✔ tune         2.0.1\n✔ modeldata    1.5.1     ✔ workflows    1.3.0\n✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n✔ recipes      1.3.1     ✔ yardstick    1.3.2\n\n\nWarning: package 'dials' was built under R version 4.5.2\n\n\nWarning: package 'infer' was built under R version 4.5.2\n\n\nWarning: package 'modeldata' was built under R version 4.5.2\n\n\nWarning: package 'parsnip' was built under R version 4.5.2\n\n\nWarning: package 'recipes' was built under R version 4.5.2\n\n\nWarning: package 'rsample' was built under R version 4.5.2\n\n\nWarning: package 'tailor' was built under R version 4.5.2\n\n\nWarning: package 'tune' was built under R version 4.5.2\n\n\nWarning: package 'workflows' was built under R version 4.5.2\n\n\nWarning: package 'workflowsets' was built under R version 4.5.2\n\n\nWarning: package 'yardstick' was built under R version 4.5.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\n#setting seed for reproducibility\nset.seed(123)\nbike_split &lt;- initial_split(daily_bike, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\n\nNow that we have created our training and test sets, we will create a 10-fold cross validation object which we can use for model tuning and evaluation at a later stage.\n\n#creating a function for CV splits\nget_cv_splits &lt;- function(data, num_folds) {\n  n &lt;- nrow(data)\n  size_fold &lt;- floor(n/num_folds)\n  random_indices &lt;- sample(n)\n  folds &lt;- vector(\"list\", num_folds)\n  \n  for (i in seq_len(num_folds)) {\n    if(i &lt; num_folds) {\n      test_idx &lt;- random_indices[((i-1)*size_fold + 1):(i*size_fold)]\n    } else {\n      test_idx &lt;- random_indices[((i-1)*size_fold + 1):n]\n    }\n    train_idx &lt;- setdiff(random_indices, test_idx)\n    folds[[i]] &lt;- list(analysis = train_idx, assessment = test_idx)\n  }\n  \n  return(folds)\n}\n\n#applying the function to generate a 10 fold CV split\nbike_folds_list &lt;- get_cv_splits(bike_train, 10)\nbike_folds &lt;- manual_rset(\n  splits = map(bike_folds_list, ~ make_splits(.x, data = bike_train)),\n  ids = paste0(\"Fold\", 1:10)\n)\n\nNow that we have our 10 fold CV splits, we can now proceed to modeling the bike sharing data.\n\n\nFitting MLR Models\nTo start with making models, we will first create several recipes. Our first recipe will be the simplest. The date variable is replaced with the weekday, weekend factor variable, as well as creating dummy variables for seasons, holidays, and the new factor variable for day type.\n\nbike_rec &lt;- recipe(total_bike_count ~ ., data = bike_train) |&gt; \n  step_date(date, features = \"dow\", keep_original_cols = FALSE) |&gt;\n  step_mutate(\n    day_type = factor(\n      if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"weekend\", \"weekday\")\n    )\n  ) |&gt;\n  step_rm(date_dow) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_dummy(all_nominal_predictors())\nbike_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"weekend\", \"weekday\"))\n\n\n• Variables removed: date_dow\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\nNow let’s create a second recipe with the same steps as above, but with interactions between seasons and holiday, seasons and temperature, temperature and rainfall.\n\nbike_rec2 &lt;- recipe(total_bike_count ~ ., data = bike_train) |&gt; \n  step_date(date, features = \"dow\", keep_original_cols = FALSE) |&gt;\n  step_mutate(\n    day_type = factor(\n      if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"weekend\", \"weekday\")\n    )\n  ) |&gt;\n  step_rm(date_dow) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_interact(terms = ~ starts_with(\"seasons_\"):starts_with(\"holiday_\")) |&gt;\n  step_interact(terms = ~ starts_with(\"seasons_\"):mean_temperature) |&gt;\n  step_interact(terms = ~ mean_temperature:total_rainfall)\nbike_rec2\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"weekend\", \"weekday\"))\n\n\n• Variables removed: date_dow\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Interactions with: starts_with(\"seasons_\"):starts_with(\"holiday_\")\n\n\n• Interactions with: starts_with(\"seasons_\"):mean_temperature\n\n\n• Interactions with: mean_temperature:total_rainfall\n\n\nOur last recipe will do everything that the 2nd recipe did but will add quadratic terms for each numeric predictor.\n\nbike_rec3 &lt;- recipe(total_bike_count ~ ., data = bike_train) |&gt; \n  step_date(date, features = \"dow\", keep_original_cols = FALSE) |&gt;\n  step_mutate(\n    day_type = factor(\n      if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"weekend\", \"weekday\")\n    )\n  ) |&gt;\n  step_rm(date_dow) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_interact(terms = ~ starts_with(\"seasons_\"):starts_with(\"holiday_\")) |&gt;\n  step_interact(terms = ~ starts_with(\"seasons_\"):mean_temperature) |&gt;\n  step_interact(terms = ~ mean_temperature:total_rainfall) |&gt;\n  step_mutate_at(\n    all_numeric_predictors(),\n    fn = list(quad = ~ .^2)\n  )\nbike_rec3\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"weekend\", \"weekday\"))\n\n\n• Variables removed: date_dow\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Interactions with: starts_with(\"seasons_\"):starts_with(\"holiday_\")\n\n\n• Interactions with: starts_with(\"seasons_\"):mean_temperature\n\n\n• Interactions with: mean_temperature:total_rainfall\n\n\n• Variable mutation for: all_numeric_predictors()\n\n\nNext, we will set up a linear model fit and use with the lm engine.\n\nlm_spec &lt;- linear_reg() |&gt; set_engine(\"lm\")\n\nNext we are going to pick a best model. In order to do this we need to fit our models using our 10 fold CV and consider the training set CV error, which will quantify which model is the best.\n\n#creating workflows for each recipe\nlm_wf1 &lt;- workflow() |&gt; add_model(lm_spec) |&gt;\n  add_recipe(bike_rec)\nlm_wf2 &lt;- workflow() |&gt; add_model(lm_spec) |&gt; \n  add_recipe(bike_rec2)\nlm_wf3 &lt;- workflow() |&gt; add_model(lm_spec) |&gt;\n  add_recipe(bike_rec3)\n\n#Fit models using the 10 fold CV\nset.seed(123)\nlm_res1 &lt;- fit_resamples(\n  lm_wf1, resamples = bike_folds, metrics = metric_set(rmse, rsq), control = control_resamples(save_pred = TRUE)\n)\nlm_res2 &lt;- fit_resamples(\n  lm_wf2, resamples = bike_folds, metrics = metric_set(rmse, rsq), control = control_resamples(save_pred = TRUE)\n)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x10\n\n\n\n\nlm_res3 &lt;- fit_resamples(\n  lm_wf3, resamples = bike_folds, metrics = metric_set(rmse, rsq), control = control_resamples(save_pred = TRUE)\n)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\n#Comparing training CV performance\ncollect_metrics(lm_res1)\n\n# A tibble: 2 × 6\n  .metric .estimator     mean     n  std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard   4116.       10 239.     pre0_mod0_post0\n2 rsq     standard      0.825    10   0.0206 pre0_mod0_post0\n\ncollect_metrics(lm_res2)\n\n# A tibble: 2 × 6\n  .metric .estimator     mean     n  std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard   3052.       10 288.     pre0_mod0_post0\n2 rsq     standard      0.907    10   0.0162 pre0_mod0_post0\n\ncollect_metrics(lm_res3)\n\n# A tibble: 2 × 6\n  .metric .estimator     mean     n   std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard   4549.       10 1246.     pre0_mod0_post0\n2 rsq     standard      0.807    10    0.0821 pre0_mod0_post0\n\n\nBased on the rmse for these models, it appears that recipe 2 lm_res2 worked best to fit the data out of these three models. Next we will fit this model to the entire training data set and get the coefficients.\n\nfinal_fit &lt;- last_fit(lm_wf2, split = bike_split)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard    3070.    pre0_mod0_post0\n2 rsq     standard       0.909 pre0_mod0_post0\n\nfinal_fit |&gt; extract_fit_parsnip() |&gt; tidy()\n\n# A tibble: 24 × 5\n   term                 estimate std.error statistic  p.value\n   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)           16797.      2117.     7.94  8.01e-14\n 2 total_rainfall        -2309.       434.    -5.31  2.46e- 7\n 3 total_snowfall          -91.5      205.    -0.447 6.56e- 1\n 4 mean_temperature       -914.      3693.    -0.247 8.05e- 1\n 5 mean_humidity         -3001.      1366.    -2.20  2.90e- 2\n 6 mean_windspeed         -461.       212.    -2.17  3.09e- 2\n 7 mean_visibility         205.       276.     0.744 4.58e- 1\n 8 mean_dewpoint          8065.      4223.     1.91  5.73e- 2\n 9 mean_solar_radiation   3212.       334.     9.62  9.42e-19\n10 seasons_Spring        -7947.      3793.    -2.10  3.72e- 2\n# ℹ 14 more rows\n\n\nFrom this, we have found the RMSE of the final model fit as well as extracted coefficients for the final model to observe the relationships predicted between rented bike counts and a variety of measured variables"
  },
  {
    "objectID": "Hw9.html#hw9-modeling-practice",
    "href": "Hw9.html#hw9-modeling-practice",
    "title": "Hw8",
    "section": "Hw9 Modeling Practice",
    "text": "Hw9 Modeling Practice\nWe will now add more models to our training set, first we will make a tuned LASSO model\n\nlasso_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt; set_engine(\"glmnet\")\nlasso_wf &lt;- workflow() |&gt; add_model(lasso_spec) |&gt; add_recipe(bike_rec)\nlasso_grid &lt;- grid_regular(penalty(range = c(-6, 0)), levels = 50)\n\nlasso_tune &lt;- tune_grid(lasso_wf, resamples = bike_folds, grid = lasso_grid, metrics = metric_set(rmse, rsq))\n\nOur next model will be a Regression Tree, following the same general steps of creating and tuning the model as above\n\ntree_spec &lt;- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) |&gt;\n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\ntree_wf &lt;- workflow() |&gt; add_model(tree_spec) |&gt; add_recipe(bike_rec)\ntree_grid &lt;- grid_random(\n  cost_complexity(range = c(-6, -1)),\n  tree_depth(),\n  min_n(),\n  size = 30\n)\n\ntree_tune &lt;- tune_grid(tree_wf, resamples = bike_folds, grid = tree_grid, metrics = metric_set(rmse, rsq))\n\nLikewise, now with a Bagged Tree model\n\nlibrary(baguette)\nbag_spec &lt;- bag_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nbag_wf &lt;- workflow() |&gt; add_model(bag_spec) |&gt; add_recipe(bike_rec)\nbag_grid &lt;- grid_random(\n  cost_complexity(range = c(-6, -1)),\n  tree_depth(),\n  min_n(),\n  size = 20\n)\n\nbag_tune &lt;- tune_grid(bag_wf, resamples = bike_folds, grid = bag_grid, metrics = metric_set(rmse, rsq))\n\nRegistered S3 method overwritten by 'butcher':\n  method                 from    \n  as.character.dev_topic generics\n\n\n→ A | warning: There was 1 warning in `dplyr::mutate()`.\n               ℹ In argument: `model = iter(...)`.\n               Caused by warning:\n               ! package 'future' was built under R version 4.5.2\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\nAgain, but now with a Random Forest model\n\nrf_spec &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() |&gt; add_model(rf_spec) |&gt; add_recipe(bike_rec)\nrf_grid &lt;- grid_random(\n  mtry(range = c(1L, 10L)),\n  min_n(range = c(2L, 50L)),\n  size = 30\n)\n\nrf_tune &lt;- tune_grid(rf_wf, resamples = bike_folds, grid = rf_grid, metrics = metric_set(rmse, rsq))\n\nNow we have created a tuned model of 4 different types, we will now select using rmse to extract the model with the best parameters\n\nbest_lasso &lt;- select_best(lasso_tune, metric = \"rmse\")\nbest_tree &lt;- select_best(tree_tune, metric = \"rmse\")\nbest_bag &lt;- select_best(bag_tune, metric = \"rmse\")\nbest_rf &lt;- select_best(rf_tune, metric = \"rmse\")\n\nNow that we have the best models selected, we will fit them to the full training data set.\n\nfinal_lasso_wf &lt;- finalize_workflow(lasso_wf, best_lasso)\nfinal_fit_lasso &lt;- fit(final_lasso_wf, data = bike_train)\n\nfinal_tree_wf &lt;- finalize_workflow(tree_wf, best_tree)\nfinal_fit_tree &lt;- fit(final_tree_wf, data = bike_train)\n\nfinal_bag_wf &lt;- finalize_workflow(bag_wf, best_bag)\nfinal_fit_bag &lt;- fit(final_bag_wf, data = bike_train)\n\nfinal_rf_wf &lt;- finalize_workflow(rf_wf, best_rf)\nfinal_fit_rf &lt;- fit(final_rf_wf, data = bike_train)\n\n#including the best MLR model from Hw 8\nfinal_fit_lm &lt;- last_fit(lm_wf2, split = bike_split)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\nNow that we have fitted all of the best models to the training set, lets evaluate their performance on the test set. To do this neatly, we will write a function that will evaluate model using the metrics of rmse and mean absolute error (mae)\n\n#function to evaluate model on test set\nevaluate_model &lt;- function(fit, name) {\n  preds &lt;- predict(fit, new_data = bike_test) |&gt;\n    bind_cols(bike_test |&gt; \n    select(total_bike_count))\n  metrics &lt;- preds |&gt; metrics(truth = total_bike_count, estimate = .pred) |&gt;\n    filter(.metric %in% c(\"rmse\", \"mae\")) |&gt;\n    mutate(model = name)\n  return(metrics)\n}\n\n#Collect performance metrics of all models\nresults_lm &lt;- collect_metrics(final_fit_lm) |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\")) |&gt;\n  mutate(model = \"Linear Regression (MLR)\")\nresults_lasso &lt;- evaluate_model(final_fit_lasso, \"LASSO Regression\")\nresults_tree &lt;- evaluate_model(final_fit_tree, \"Regression Tree\")\nresults_bag &lt;- evaluate_model(final_fit_bag, \"Bagged Tree\")\nresults_rf &lt;- evaluate_model(final_fit_rf, \"Random Forest\")\n\n#Combine and arrange the results for ease of viewing\nall_results &lt;- bind_rows(results_lm, results_lasso, results_tree, results_bag, results_rf)\nall_results_summary &lt;- all_results |&gt; arrange(.metric)\nprint(all_results_summary)\n\n# A tibble: 9 × 5\n  .metric .estimator .estimate .config         model                  \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;                  \n1 mae     standard       3157. &lt;NA&gt;            LASSO Regression       \n2 mae     standard       2339. &lt;NA&gt;            Regression Tree        \n3 mae     standard       2150. &lt;NA&gt;            Bagged Tree            \n4 mae     standard       2166. &lt;NA&gt;            Random Forest          \n5 rmse    standard       3070. pre0_mod0_post0 Linear Regression (MLR)\n6 rmse    standard       4060. &lt;NA&gt;            LASSO Regression       \n7 rmse    standard       3130. &lt;NA&gt;            Regression Tree        \n8 rmse    standard       2953. &lt;NA&gt;            Bagged Tree            \n9 rmse    standard       2759. &lt;NA&gt;            Random Forest          \n\n\nBased on this comparison, the model with both the lowest rmse and mae is the random forest model, making it the overall best model.\nNext we will extract the final model fits and display them in differing ways depending on the model\n\n#Extract final coefficients (MLR and LASSO Model)\nfinal_mlr &lt;- final_fit |&gt; extract_fit_parsnip()\nmlr_coefs &lt;- tidy(final_mlr)\nmlr_coefs\n\n# A tibble: 24 × 5\n   term                 estimate std.error statistic  p.value\n   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)           16797.      2117.     7.94  8.01e-14\n 2 total_rainfall        -2309.       434.    -5.31  2.46e- 7\n 3 total_snowfall          -91.5      205.    -0.447 6.56e- 1\n 4 mean_temperature       -914.      3693.    -0.247 8.05e- 1\n 5 mean_humidity         -3001.      1366.    -2.20  2.90e- 2\n 6 mean_windspeed         -461.       212.    -2.17  3.09e- 2\n 7 mean_visibility         205.       276.     0.744 4.58e- 1\n 8 mean_dewpoint          8065.      4223.     1.91  5.73e- 2\n 9 mean_solar_radiation   3212.       334.     9.62  9.42e-19\n10 seasons_Spring        -7947.      3793.    -2.10  3.72e- 2\n# ℹ 14 more rows\n\nfinal_lasso &lt;- final_fit_lasso |&gt; extract_fit_parsnip()\nlasso_coefs &lt;- tidy(final_lasso)\n\nWarning: package 'glmnet' was built under R version 4.5.2\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-10\n\nlasso_coefs\n\n# A tibble: 14 × 3\n   term                 estimate  penalty\n   &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)            19600. 0.000001\n 2 total_rainfall         -1778. 0.000001\n 3 total_snowfall          -289. 0.000001\n 4 mean_temperature        1426. 0.000001\n 5 mean_humidity          -1221. 0.000001\n 6 mean_windspeed          -560. 0.000001\n 7 mean_visibility         -132. 0.000001\n 8 mean_dewpoint           3212. 0.000001\n 9 mean_solar_radiation    3894. 0.000001\n10 seasons_Spring         -5599. 0.000001\n11 seasons_Summer         -4898. 0.000001\n12 seasons_Winter         -8277. 0.000001\n13 holiday_No.Holiday      3325. 0.000001\n14 day_type_weekend       -2367. 0.000001\n\n#Plotting the final tree\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.5.2\n\n\nLoading required package: rpart\n\n\n\nAttaching package: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nfinal_tree &lt;- final_fit_tree |&gt; extract_fit_parsnip()\nrpart.plot(final_tree$fit, fallen.leaves = TRUE, cex = 0.7)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n#Variable importance plot for bagged tree model\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nfinal_bag &lt;- final_fit_bag |&gt; extract_fit_parsnip()\n\n#baking the training data so the columns match what the model expects\nrec &lt;- final_fit_bag |&gt; extract_recipe()\ntrain_processed &lt;- bake(rec, new_data = bike_train)\n\n#seperating predictors and outcomes\ntrain_x &lt;- train_processed |&gt; select(-total_bike_count)\ntrain_y &lt;- train_processed$total_bike_count\n\n#continuing to the variable importance plot for the bagged tree model\nbag_pred &lt;- function(object, newdata) {\n  predict(object, new_data = newdata) |&gt;\n    pull(.pred)\n}\nvip(final_bag, method = \"permute\", train = train_x, target = train_y, metric = \"rmse\", pred_wrapper = bag_pred, num_features = 15)\n\n\n\n\n\n\n\n#Variable importance plot for random forest model\nfinal_rf &lt;- final_fit_rf |&gt; extract_fit_parsnip()\nrf_model &lt;- final_rf$fit\nrf_importance &lt;- tibble(\n  variable = names(rf_model$variable.importance),\n  importance = rf_model$variable.importance\n) |&gt;\n  arrange(desc(importance))\n\nrf_importance |&gt;\n  ggplot(aes(x = importance, y = reorder(variable, importance))) + geom_col() + labs(title = \"Random Forest Variable Importance\", x = \"Importance\", y = \"Variable\") + theme_minimal()\n\n\n\n\n\n\n\n\nLastly, we will fit the random forest model (the best one) to all of the data, and look at the variable importance plot.\n\nfinal_rf_all &lt;- final_rf_wf|&gt; \n  fit(data = daily_bike)\nrf_final_fit &lt;- final_rf_all |&gt; extract_fit_parsnip()\nrf_final_fit\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~10L,      x), num.trees = ~1000, min.node.size = min_rows(~11L, x),      importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             10 \nTarget node size:                 11 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       7729407 \nR squared (OOB):                  0.9217253 \n\nrf_final_importance &lt;- tibble(\n  variable = names(rf_final_fit$fit$variable.importance), importance = rf_final_fit$fit$variable.importance\n) |&gt; \n  arrange(desc(importance))\n\nrf_final_importance |&gt;\n  ggplot(aes(x = importance, y = reorder(variable, importance))) + geom_col() + labs(title = \"Random Forest Variable Importance\", x = \"Importance\", y = \"Variable\") + theme_minimal()"
  }
]